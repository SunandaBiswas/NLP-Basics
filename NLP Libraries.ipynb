{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "506c8b47",
   "metadata": {},
   "source": [
    "# NLP Library : NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d20e734c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\asus\\anaconda3\\lib\\site-packages (3.6.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\asus\\anaconda3\\lib\\site-packages (from nltk) (4.59.0)\n",
      "Requirement already satisfied: regex in c:\\users\\asus\\anaconda3\\lib\\site-packages (from nltk) (2021.4.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\asus\\anaconda3\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: click in c:\\users\\asus\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ebe5fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize  #for tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d8a16a",
   "metadata": {},
   "source": [
    "##### Here, the given text is being tokenized using word_tokenize() function of NLTK, that splits the sentence by whitespace. The output is a list that is the first step to converting text data into numerical forms(Vectorization). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818d3d70",
   "metadata": {},
   "source": [
    "##### TOKENIZATION : Splits the entire text document into smaller units called tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74a4082e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'ocean', 'is', 'a', 'huge', 'body', 'of', 'saltwater', 'that', 'covers', 'about', '71', 'percent', 'of', 'Earth', '’', 's', 'surface', '.', 'The', 'planet', 'has', 'one', 'global', 'ocean', ',', 'though', 'oceanographers', 'and', 'the', 'countries', 'of', 'the', 'world', 'have', 'traditionally', 'divided', 'it', 'into', 'four', 'distinct', 'regions', ':', 'the', 'Pacific', ',', 'Atlantic', ',', 'Indian', ',', 'and', 'Arctic', 'oceans', '.', 'Beginning', 'in', 'the', '20th', 'century', ',', 'some', 'oceanographers', 'labeled', 'the', 'seas', 'around', 'Antarctica', 'the', 'Southern', 'Ocean', ',', 'and', 'in', '2021', 'National', 'Geographic', 'officially', 'recognized', 'this', 'fifth', 'ocean', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"The ocean is a huge body of saltwater that covers about 71 percent of Earth’s surface. The planet has one global ocean, though oceanographers and the countries of the world have traditionally divided it into four distinct regions: the Pacific, Atlantic, Indian, and Arctic oceans. Beginning in the 20th century, some oceanographers labeled the seas around Antarctica the Southern Ocean, and in 2021 National Geographic officially recognized this fifth ocean.\"\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64e6391",
   "metadata": {},
   "source": [
    "#### Now, in the tokenized list, there are stopwords, it is required to remove the stopwords through data cleaning process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2b3c32",
   "metadata": {},
   "source": [
    "##### STOPWORDS: The Connector words or the most common words, do not contribute to the meaning of the sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cfbfdbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "print(stopwords)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
